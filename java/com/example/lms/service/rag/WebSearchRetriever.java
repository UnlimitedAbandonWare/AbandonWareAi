package com.example.lms.service.rag;

import com.example.lms.search.TraceStore;
import com.example.lms.service.guard.GuardContextHolder;
import com.example.lms.search.provider.WebSearchProvider;
import com.example.lms.service.rag.pre.QueryContextPreprocessor;
import com.example.lms.util.HtmlTextUtil;
import dev.langchain4j.data.document.Metadata;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.rag.content.Content;
import dev.langchain4j.rag.content.retriever.ContentRetriever;
import dev.langchain4j.rag.query.Query;
import lombok.RequiredArgsConstructor;
import java.util.List;
import com.example.lms.service.rag.filter.GenericDocClassifier;
import com.example.lms.service.rag.detector.GameDomainDetector;
import com.example.lms.service.rag.filter.EducationDocClassifier;
import org.slf4j.LoggerFactory;
import org.slf4j.Logger;

import java.util.regex.Pattern; /* ğŸ”´ NEW */

@org.springframework.stereotype.Component
public class WebSearchRetriever implements ContentRetriever {

    private static final Logger log = LoggerFactory.getLogger(WebSearchRetriever.class);
    private final WebSearchProvider webSearchProvider;
    /**
     * Aggregate web search across multiple providers. This component fans
     * out to the configured {@link com.acme.aicore.domain.ports.WebSearchProvider}
     * implementations (Bing/Naver/Brave) in priority order and merges the
     * results. When present it allows the web retrieval stage to fall
     * back to additional providers when the primary Naver results are
     * insufficient. It is optional and may be null when no providers
     * are configured.
     */
    private final com.acme.aicore.adapters.search.CachedWebSearch multiSearch;
    // ìŠ¤í”„ë§ í”„ë¡œí¼í‹°ë¡œ ì£¼ì…(ìƒì„±ì ì£¼ì…ì˜ int ë¹ˆ ë¬¸ì œ íšŒí”¼)
    @org.springframework.beans.factory.annotation.Value("${rag.search.top-k:5}")
    private int topK;

    @org.springframework.beans.factory.annotation.Autowired(required = false)
    private QueryContextPreprocessor preprocessor;

    @org.springframework.beans.factory.annotation.Value("${privacy.boundary.block-web-search:false}")
    private boolean blockWebSearch;

    @org.springframework.beans.factory.annotation.Value("${privacy.boundary.block-web-search-on-sensitive:false}")
    private boolean blockWebSearchOnSensitive;

    private final com.example.lms.service.rag.extract.PageContentScraper pageScraper;
    // ìµœì†Œ 3ê°œ ì´ìƒì˜ ìŠ¤ë‹ˆí«ì„ ìœ ì§€í•´ LLM ì»¨í…ìŠ¤íŠ¸ë¥¼ í’ë¶€í•˜ê²Œ í•œë‹¤.
    private static final int MIN_SNIPPETS = 3;
    // ë„ë©”ì¸ ì‹ ë¢°ë„ ì ìˆ˜ë¡œ ì •ë ¬ ê°€ì¤‘
    private final com.example.lms.service.rag.auth.AuthorityScorer authorityScorer;
    // ë²”ìš© íŒì •ê¸°ëŠ” ì£¼ì…ë°›ì•„ ë„ë©”ì¸ë³„ë¡œ ë™ì‘í•˜ë„ë¡ í•œë‹¤.
    private final GenericDocClassifier genericClassifier;
    // ì§ˆì˜ ë„ë©”ì¸ ì¶”ì •ê¸°
    private final com.example.lms.service.rag.detector.GameDomainDetector domainDetector;
    // êµìœ¡ í† í”½ ë¶„ë¥˜ê¸°: êµìœ¡ ë„ë©”ì¸ì¼ ë•Œ ìŠ¤ë‹ˆí« í•„í„°ë§ì— ì‚¬ìš©ëœë‹¤.
    private final com.example.lms.service.rag.filter.EducationDocClassifier educationClassifier;
    private static final Pattern META_TAG = Pattern.compile("\\[[^\\]]+\\]");
    private static final Pattern TIME_TAG = Pattern.compile("\\b\\d{1,2}:\\d{2}\\b");
    /* ğŸ”µ ë´‡/ìº¡ì°¨ í˜ì´ì§€ íŒíŠ¸ */
    /* ë“±ì—ì„œ ë°˜í™˜ë˜ëŠ” ìº¡ì°¨/ë´‡ ì°¨ë‹¨ íŒíŠ¸ ì œê±°ìš© */
    private static final Pattern CAPTCHA_HINT = Pattern.compile(
            "(?i)(captcha|are you (a )?robot|unusual\\s*traffic|verify you are human|\\.com/captcha|bots\\s*use\\s*)");

    // Extract site: filters from a query (e.g., "site:wikipedia.org").
    // We reuse these for "cheap retry" where we want to filter already-prefetched
    // SERP
    // results instead of doing additional external calls.
    private static final Pattern SITE_FILTER = Pattern.compile("(?i)\\bsite:([^\\s\\)]+)");

    private static final String SERP_CACHE_TRACE_KEY = "webSearch.serpCache";
    private static final int SERP_CACHE_MAX = 32;

    public WebSearchRetriever(
            WebSearchProvider webSearchProvider,
            com.acme.aicore.adapters.search.CachedWebSearch multiSearch,
            com.example.lms.service.rag.extract.PageContentScraper pageScraper,
            com.example.lms.service.rag.auth.AuthorityScorer authorityScorer,
            GenericDocClassifier genericClassifier,
            com.example.lms.service.rag.detector.GameDomainDetector domainDetector,
            com.example.lms.service.rag.filter.EducationDocClassifier educationClassifier) {
        this.webSearchProvider = webSearchProvider;
        this.multiSearch = multiSearch;
        this.pageScraper = pageScraper;
        this.authorityScorer = authorityScorer;
        this.genericClassifier = genericClassifier;
        this.domainDetector = domainDetector;
        this.educationClassifier = educationClassifier;
    }

    private static String normalize(String raw) { /* ğŸ”´ NEW */
        if (raw == null)
            return "";

        String s = META_TAG.matcher(raw).replaceAll("");
        s = TIME_TAG.matcher(s).replaceAll("");
        return s.replace("\n", " ").trim();
    }

    /**
     * Extract a version token from the query string. A version is defined
     * as two numeric components separated by a dot or middot character. If
     * no such token is present, {@code null} is returned.
     *
     * @param q the query text
     * @return the extracted version (e.g. "5.8") or null
     */
    private static String extractVersion(String q) {
        if (q == null)
            return null;
        java.util.regex.Matcher m = java.util.regex.Pattern.compile("(\\d+)[\\.Â·](\\d+)").matcher(q);
        return m.find() ? (m.group(1) + "." + m.group(2)) : null;
    }

    /**
     * Build a regex that matches the exact version token in text. Dots in
     * the version are replaced with a character class that matches dot or
     * middot to handle variations in punctuation. Anchors ensure that
     * longer numbers containing the version as a substring are not falsely
     * matched.
     *
     * @param v the version string (e.g. "5.8")
     * @return a compiled regex pattern matching the exact token
     */
    private static java.util.regex.Pattern versionRegex(String v) {
        String core = v.replace(".", "[\\.Â·\\s]");
        return java.util.regex.Pattern.compile("(?<!\\d)" + core + "(?!\\d)");
    }

    /* âœ… ì„ í˜¸ ë„ë©”ì¸: ì œê±°ê°€ ì•„ë‹Œ 'ìš°ì„  ì •ë ¬'ë§Œ ìˆ˜í–‰ */
    private static final List<String> PREFERRED = List.of(
            // ê³µì‹/ê¶Œìœ„
            "genshin.hoyoverse.com", "hoyoverse.com", "hoyolab.com",
            "wikipedia.org", "eulji.ac.kr", "ac.kr", "go.kr",
            // í•œêµ­ ì»¤ë®¤ë‹ˆí‹°Â·ë¸”ë¡œê·¸(ì‚­ì œ X, ë‹¨ì§€ í›„ìˆœìœ„)
            "namu.wiki", "blog.naver.com");

    private static boolean containsPreferred(String s) {
        return PREFERRED.stream().anyMatch(s::contains);
    }

    @Override
    public List<Content> retrieve(Query query) {
        String normalized = normalize(query != null ? query.text() : "");

        java.util.Map<String, Object> meta = toMetaMap(query);
        meta.putIfAbsent("purpose", "WEB_SEARCH");

        var gctx = GuardContextHolder.get();
        boolean sensitive = gctx != null && gctx.isSensitiveTopic();
        boolean planBlockAll = gctx != null && gctx.planBool("privacy.boundary.block-web-search", false);
        boolean planBlockOnSensitive = gctx != null
                && gctx.planBool("privacy.boundary.block-web-search-on-sensitive", false);
        if (blockWebSearch || planBlockAll || (sensitive && (blockWebSearchOnSensitive || planBlockOnSensitive))) {
            TraceStore.put("privacy.web.blocked", true);
            return java.util.Collections.emptyList();
        }

        if (preprocessor != null) {
            try {
                normalized = preprocessor.enrich(normalized, meta);
            } catch (Exception e) {
                log.debug("[WebSearchRetriever] preprocessor failed: {}", e.toString());
            }
        }

        // If the caller appended "site:" filters (e.g., detour/cheap-retry or
        // user-specified),
        // try to reuse the base SERP snippets we already have (prefetch/trace cache)
        // instead of triggering a new
        // external search call. This helps prevent "web=0" starvation loops when a base
        // SERP was already fetched.
        final java.util.List<String> requestedSites = extractSiteFilters(normalized);
        final boolean hasSiteFilters = requestedSites != null && !requestedSites.isEmpty();
        final String baseQueryKey = canonicalBaseQuery(normalized);
        // ì¿¼ë¦¬ ë„ë©”ì¸ ì¶”ì •: null ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•˜ì—¬ GENERAL ê¸°ë³¸ê°’ ì‚¬ìš©
        String domain = domainDetector != null ? domainDetector.detect(normalized) : "GENERAL";
        boolean isGeneral = "GENERAL".equalsIgnoreCase(domain);

        int reqTopK = metaInt(meta, "webTopK", this.topK);
        long webBudgetMs = metaLong(meta, "webBudgetMs", -1L);
        boolean allowWeb = metaBool(meta, "allowWeb", true);
        if (!allowWeb) {
            return java.util.Collections.emptyList();
        }

        int k = Math.max(reqTopK, MIN_SNIPPETS);
        int maxAttempts = (webBudgetMs > 0 ? (webBudgetMs <= 1500 ? 1 : (webBudgetMs <= 3000 ? 2 : 3)) : 3);

        // Extract a version token from the query. When present, enforce that
        // each snippet contains the exact version. This helps prevent
        // contamination from neighbouring versions (e.g. 5.7 or 5.9) when the
        // user asks about a specific patch.
        String ver = extractVersion(normalized);
        java.util.regex.Pattern must = (ver != null) ? versionRegex(ver) : null;
        // 1) 1ì°¨ ìˆ˜ì§‘: (prefetchê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©) â†’ ì—†ìœ¼ë©´ topK*2 â†’ ì¤‘ë³µ/ì •ë ¬ í›„ topK
        //
        // Additionally: when the caller added site: filters, try to reuse the same base
        // SERP snippets
        // (prefetch or request-scoped cache) by filtering them by the requested sites.
        boolean usedPrefetched = false;
        boolean reusedBaseSerp = false;
        String reusedBaseSerpSource = null;
        List<String> first = null;

        Object pq = meta.get("prefetch.web.query");
        Object ps = meta.get("prefetch.web.snippets");
        java.util.List<String> prefetchSnips = java.util.Collections.emptyList();
        String prefetchKeyNorm = null;

        if (pq != null && ps instanceof java.util.List<?> raw) {
            prefetchKeyNorm = normalize(String.valueOf(pq));
            java.util.List<String> out = new java.util.ArrayList<>();
            for (Object o : raw) {
                if (o == null)
                    continue;
                String s = String.valueOf(o).trim();
                if (!s.isBlank())
                    out.add(s);
            }
            prefetchSnips = out;

            // Exact-match prefetch (unchanged behaviour)
            if (!prefetchKeyNorm.isBlank() && prefetchKeyNorm.equalsIgnoreCase(normalized) && !out.isEmpty()) {
                usedPrefetched = true;
                first = out;
                reusedBaseSerpSource = "prefetch.exact";
            }
        }

        // Base-key prefetch/cache reuse for "site:" filters:
        // - Filter the already-fetched base SERP (prefetch or request-scoped cache)
        // - If the filtered set is large enough, skip a new external call
        // - Otherwise, keep it as a seed and fall back to a real site-filtered search
        java.util.List<String> siteReuseSeed = null;
        final int minSiteFilteredDocsToSkipSearch = computeSiteFilterMinDocsToSkipSearch(meta, k);
        com.example.lms.search.TraceStore.put("webSearch.siteFilterMinDocsToSkipSearch",
                minSiteFilteredDocsToSkipSearch);

        // Base-key prefetch reuse for site: filters
        if (first == null && hasSiteFilters && prefetchKeyNorm != null && !prefetchKeyNorm.isBlank()
                && !prefetchSnips.isEmpty()) {
            String prefetchBaseKey = canonicalBaseQuery(prefetchKeyNorm);
            if (!prefetchBaseKey.isBlank() && prefetchBaseKey.equalsIgnoreCase(baseQueryKey)) {
                java.util.List<String> filtered = filterSnippetsBySites(prefetchSnips, requestedSites, k * 2);
                if (!filtered.isEmpty()) {
                    reusedBaseSerp = true;
                    if (filtered.size() >= minSiteFilteredDocsToSkipSearch) {
                        usedPrefetched = true;
                        reusedBaseSerpSource = "prefetch.base+siteFilter:skip";
                        com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseMode", "skip");
                        first = filtered;
                    } else {
                        reusedBaseSerpSource = "prefetch.base+siteFilter:seed";
                        com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseMode", "seed");
                        siteReuseSeed = filtered;
                    }
                }
            }
        }

        // Request-scoped SERP cache reuse for site: filters
        if (first == null && hasSiteFilters && !baseQueryKey.isBlank()) {
            java.util.List<String> cachedBase = serpCacheGet(baseQueryKey);
            if (cachedBase != null && !cachedBase.isEmpty()) {
                java.util.List<String> filtered = filterSnippetsBySites(cachedBase, requestedSites, k * 2);
                if (!filtered.isEmpty()) {
                    reusedBaseSerp = true;
                    if (filtered.size() >= minSiteFilteredDocsToSkipSearch) {
                        usedPrefetched = true;
                        reusedBaseSerpSource = "trace.base+siteFilter:skip";
                        com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseMode", "skip");
                        first = filtered;
                    } else {
                        reusedBaseSerpSource = "trace.base+siteFilter:seed";
                        com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseMode", "seed");
                        if (siteReuseSeed == null || siteReuseSeed.isEmpty()) {
                            siteReuseSeed = filtered;
                        } else {
                            java.util.LinkedHashSet<String> merged = new java.util.LinkedHashSet<>(siteReuseSeed);
                            merged.addAll(filtered);
                            siteReuseSeed = merged.stream().limit(k * 2L).toList();
                        }
                    }
                }
            }
        }

        if (!usedPrefetched) {
            java.util.List<String> searched = searchWithAggressiveRetry(
                    normalized,
                    k * 2,
                    must,
                    maxAttempts,
                    webBudgetMs);

            if (siteReuseSeed != null && !siteReuseSeed.isEmpty()) {
                com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseSeedCount", siteReuseSeed.size());
                if (searched == null || searched.isEmpty()) {
                    first = siteReuseSeed;
                } else {
                    java.util.LinkedHashSet<String> merged = new java.util.LinkedHashSet<>();
                    merged.addAll(siteReuseSeed);
                    merged.addAll(searched);
                    first = new java.util.ArrayList<>(merged);
                    com.example.lms.search.TraceStore.put("webSearch.siteFilterReuseMerged", true);
                }
            } else {
                first = searched;
            }
        }

        // Persist the base SERP snippets for later site-filter reuse within the same
        // request.
        // Never cache "site:"-filtered queries to avoid polluting the base cache.
        if (!hasSiteFilters && first != null && !first.isEmpty() && !baseQueryKey.isBlank()) {
            serpCachePut(baseQueryKey, first);
        }

        com.example.lms.search.TraceStore.put("webSearch.reusedBaseSerp", reusedBaseSerp);
        com.example.lms.search.TraceStore.put("webSearch.reusedBaseSerpSource", reusedBaseSerpSource);

        // ğŸš€ Fan-out to additional providers via CachedWebSearch. When the
        // primary Naver results are fewer than the desired count, fetch
        // supplementary snippets from other providers (e.g. Bing/Brave). The
        // CachedWebSearch component merges provider responses according to
        // provider priorities and caches the result. Failures are
        // intentionally swallowed to avoid impacting the main retrieval.
        List<String> supplemental = java.util.Collections.emptyList();
        // Only fan-out when the primary provider returned fewer than the desired count.
        if (multiSearch != null && (first == null || first.size() < k)
                && (!usedPrefetched
                        || (hasSiteFilters && reusedBaseSerpSource != null
                                && reusedBaseSerpSource.contains("siteFilter:skip"))
                        || metaBool(meta, "web.multiSearch.allowWhenPrefetched", false))) {
            try {
                var q = new com.acme.aicore.domain.model.WebSearchQuery(normalized);
                // [Patch] Limit fanout to two providers (Naver, Brave) and
                // allow up to 5 seconds to account for network variability.
                var bundle = multiSearch.searchMulti(q, 2)
                        .block(java.time.Duration
                                .ofMillis(webBudgetMs > 0 ? Math.min(5000L, Math.max(600L, webBudgetMs)) : 5000L));
                if (bundle != null && bundle.docs() != null) {
                    supplemental = bundle.docs().stream()
                            .map(d -> {
                                String core = (d.title() + " - " + d.snippet()).trim();
                                return core.isBlank() ? d.url() : core;
                            })
                            .filter(s -> s != null && !s.isBlank())
                            .toList();
                }
            } catch (Exception e) {
                // ignore errors; supplemental remains empty
            }
        }

        // Prepend the supplemental results to the primary list, ensuring
        // duplicates are removed while preserving order. This prioritises
        // provider results before applying ranking heuristics below. Only
        // the first (topK*2) snippets are considered to limit memory usage.
        if (supplemental != null && !supplemental.isEmpty()) {
            java.util.LinkedHashSet<String> combined = new java.util.LinkedHashSet<>();
            combined.addAll(supplemental);
            combined.addAll(first);
            first = combined.stream().limit(k * 2).toList();
        }

        // Refresh the request-scoped SERP cache with the widest (supplemental+first)
        // list.
        // This gives the "cheap retry" site-filter path more chances to find host
        // matches
        // without triggering a new search.
        if (!hasSiteFilters && first != null && !first.isEmpty() && !baseQueryKey.isBlank()) {
            serpCachePut(baseQueryKey, first);
        }
        if (log.isDebugEnabled()) {
            log.debug("[WebSearchRetriever] first raw={} (q='{}')", first.size(), normalized);
        }
        // ì„ í˜¸+ ë„ë©”ì¸ Authority ê°€ì¤‘ ì •ë ¬(ì‚­ì œ ì•„ë‹˜). ë²”ìš© í˜ë„í‹°ëŠ” GENERAL/EDUCATION ë„ë©”ì¸ì—ì„œëŠ” ì œê±°
        List<String> ranked = first.stream()
                .distinct()
                .sorted((a, b) -> {
                    double aw = authorityScorer.weightFor(extractUrl(a))
                            - (isGeneral ? 0.0 : genericClassifier.penalty(a, domain));
                    double bw = authorityScorer.weightFor(extractUrl(b))
                            - (isGeneral ? 0.0 : genericClassifier.penalty(b, domain));
                    int cmp = Double.compare(bw, aw); // high first (penalty ë°˜ì˜)
                    if (cmp != 0)
                        return cmp;
                    // ë™ë¥ ì´ë©´ ì„ í˜¸ ë„ë©”ì¸ ìš°ì„ 
                    return Boolean.compare(containsPreferred(b), containsPreferred(a));
                })
                .limit(k)
                .toList();
        // ë²”ìš© ìŠ¤ë‹ˆí« ì»·: ë„ë©”ì¸ íŠ¹í™”(ì˜ˆ: GENSHIN/EDU)ì—ì„œë§Œ ì ìš©
        // - ë‹¨, í•„í„° ì´í›„ ê²°ê³¼ê°€ 0ê°œê°€ ë˜ë©´("starved") ê²°ì„ ì´ ëŠê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ FAIL-SOFTë¡œ ë³µêµ¬
        if (!isGeneral && genericClassifier != null) {
            List<String> beforeGenericCut = ranked;
            List<String> afterGenericCut = ranked.stream()
                    .filter(s -> !genericClassifier.isGenericSnippet(s, domain))
                    .limit(k)
                    .toList();
            if (afterGenericCut.isEmpty() && !beforeGenericCut.isEmpty()) {
                TraceStore.put("webSearch.genericCut.starved", true);
                ranked = beforeGenericCut;
            } else {
                ranked = afterGenericCut;
            }
        }

        // 2) í´ë°±: ì§€ë‚˜ì¹œ ê³µì†ì–´/í˜¸ì¹­ ì •ë¦¬
        List<String> fallback = (usedPrefetched || ranked.size() >= MIN_SNIPPETS) ? List.of()
                : webSearchProvider.search(normalized.replace("êµìˆ˜ë‹˜", "êµìˆ˜").replace("ë‹˜", ""), k);

        List<String> finalSnippets = java.util.stream.Stream.of(ranked, fallback)
                .flatMap(java.util.Collection::stream)
                .distinct()
                .limit(k)
                .toList();

        // If the detected domain is EDUCATION, apply an education topic filter
        // to remove snippets that are unrelated to education/academy topics. This
        // leverages the EducationDocClassifier to detect whether a snippet is
        // genuinely about education. Without this filter generic or noise
        // snippets from pet or automotive sites may contaminate the retrieval.
        if ("EDUCATION".equalsIgnoreCase(domain) && educationClassifier != null) {
            List<String> beforeEduCut = finalSnippets;
            List<String> afterEduCut = finalSnippets.stream()
                    .filter(s -> {
                        try {
                            return educationClassifier.isEducation(s);
                        } catch (Exception e) {
                            return true;
                        }
                    })
                    .limit(k)
                    .toList();

            // Fail-soft: if the classifier ends up filtering out everything,
            // keep the original list to avoid "fused=0" starvation.
            if (afterEduCut.isEmpty() && !beforeEduCut.isEmpty()) {
                TraceStore.put("webSearch.eduCut.starved", true);
                finalSnippets = beforeEduCut;
            } else {
                finalSnippets = afterEduCut;
            }
        }

        if (log.isDebugEnabled()) {
            log.debug("[WebSearchRetriever] selected={} (topK={})", finalSnippets.size(), topK);
        }
        // 3) ê° ê²°ê³¼ì˜ URL ë³¸ë¬¸ì„ ì½ì–´ â€˜ì§ˆë¬¸-ìœ ì‚¬ë„â€™ë¡œ í•µì‹¬ ë¬¸ë‹¨ ì¶”ì¶œ
        String providerName = null;
        try {
            providerName = (webSearchProvider != null) ? webSearchProvider.getName() : null;
        } catch (Throwable ignore) {
            // ignore
        }
        if (providerName == null || providerName.isBlank()) {
            providerName = "web";
        }

        java.util.List<Content> out = new java.util.ArrayList<>();
        for (String s : finalSnippets) {
            String url = extractUrl(s); // â¬…ï¸ ì—†ë˜ util ë©”ì„œë“œ ì¶”ê°€(ì•„ë˜)
            if (url == null || CAPTCHA_HINT.matcher(s).find()) { // ğŸ”’ ì˜ì‹¬ ë¼ì¸ ìŠ¤í‚µ
                out.add(toWebContent(s, url, providerName)); // URL ì—†ìŒ â†’ ê¸°ì¡´ ìŠ¤ë‹ˆí« ì‚¬ìš©
                continue;
            }
            try {
                String body = pageScraper.fetchText(url, /* timeoutMs */6000);
                // SnippetPrunerëŠ” (String, String) ì‹œê·¸ë‹ˆì²˜ë§Œ ì¡´ì¬ â†’ ë‹¨ì¼ ê²°ê³¼ë¡œ ì²˜ë¦¬
                // ğŸ”µ ìš°ë¦¬ ìª½ ê°„ë‹¨ ë”¥ ìŠ¤ë‹ˆí« ì¶”ì¶œ(ì„ë² ë”© ì—†ì´ í‚¤ì›Œë“œ/ê¸¸ì´ ê¸°ë°˜)
                String picked = pickByHeuristic(query.text(), body, 480);
                if (picked == null || picked.isBlank()) {
                    out.add(toWebContent(s, url, providerName));
                } else {
                    out.add(toWebContent(picked + "\n\n[ì¶œì²˜] " + url, url, providerName));
                }
            } catch (Exception e) {
                log.debug("[WebSearchRetriever] scrape fail {} â†’ fallback snippet", url);
                out.add(toWebContent(s, url, providerName));
            }
        }

        // Optional authorityMin filter (used by needle/probe stage). This is
        // fail-soft unless strict=true.
        // meta already defined at line 144, reuse the existing variable
        double authorityMin = metaDouble(meta, "web.authorityMin", -1.0);
        boolean strict = metaBool(meta, "web.authorityMin.strict", false);
        if (authorityMin > 0.0d) {
            int before = out.size();
            java.util.List<Content> filtered = out.stream()
                    .filter(c -> {
                        try {
                            String u = extractUrl(c.textSegment().text());
                            if (u == null || u.isBlank())
                                return false;
                            return authorityScorer.weightFor(u) >= authorityMin;
                        } catch (Exception e) {
                            return false;
                        }
                    })
                    .toList();

            // If strict OR we still have a meaningful number of snippets, keep the filtered
            // set.
            if (strict || filtered.size() >= Math.min(MIN_SNIPPETS, before)) {
                out = new java.util.ArrayList<>(filtered);
                TraceStore.put("webSearch.authorityMin", authorityMin);
                TraceStore.put("webSearch.authorityMin.strict", strict);
                TraceStore.put("webSearch.authorityMin.before", before);
                TraceStore.put("webSearch.authorityMin.after", out.size());
            } else {
                TraceStore.put("webSearch.authorityMin.skipped", Boolean.TRUE);
            }
        }
        return out.stream().limit(k).toList();
    }

    @SuppressWarnings("unchecked")
    private static java.util.Map<String, Object> toMetaMap(Query query) {
        if (query == null || query.metadata() == null)
            return java.util.Collections.emptyMap();
        Object meta = query.metadata();
        if (meta instanceof java.util.Map<?, ?> raw) {
            java.util.Map<String, Object> out = new java.util.HashMap<>();
            for (java.util.Map.Entry<?, ?> e : raw.entrySet()) {
                if (e.getKey() != null)
                    out.put(String.valueOf(e.getKey()), e.getValue());
            }
            return out;
        }
        try {
            java.lang.reflect.Method m = meta.getClass().getMethod("asMap");
            Object v = m.invoke(meta);
            if (v instanceof java.util.Map<?, ?> m2) {
                java.util.Map<String, Object> out = new java.util.HashMap<>();
                for (java.util.Map.Entry<?, ?> e : m2.entrySet()) {
                    if (e.getKey() != null)
                        out.put(String.valueOf(e.getKey()), e.getValue());
                }
                return out;
            }
        } catch (NoSuchMethodException ignore) {
            try {
                java.lang.reflect.Method m = meta.getClass().getMethod("map");
                Object v = m.invoke(meta);
                if (v instanceof java.util.Map<?, ?> m2) {
                    java.util.Map<String, Object> out = new java.util.HashMap<>();
                    for (java.util.Map.Entry<?, ?> e : m2.entrySet()) {
                        if (e.getKey() != null)
                            out.put(String.valueOf(e.getKey()), e.getValue());
                    }
                    return out;
                }
            } catch (Exception ignore2) {
                return java.util.Collections.emptyMap();
            }
        } catch (Exception ignore) {
            return java.util.Collections.emptyMap();
        }
        return java.util.Collections.emptyMap();
    }

    /**
     * Computes the minimum number of docs required to skip an actual
     * (site-filtered) search when
     * we can instead reuse a cached/base SERP and filter it by host.
     *
     * Policy goal:
     * - Avoid the "0-doc / low-doc" loops when site filters are present by only
     * skipping when
     * we already have enough documents to satisfy both citation needs and rerank
     * needs.
     * - Still allow callers (e.g. guard detour cheap-retry) to override the
     * threshold via
     * {@code siteFilter.minDocsToSkipSearch}.
     */
    private static int computeSiteFilterMinDocsToSkipSearch(java.util.Map<String, Object> meta, int k) {
        int explicit = metaInt(meta, "siteFilter.minDocsToSkipSearch", -1);
        if (explicit > 0) {
            return Math.min(k, Math.max(1, explicit));
        }

        // The guard layer may pass the required citation count through metadata.
        int minCitations = metaInt(meta, "minCitations",
                metaInt(meta, "citationMin", metaInt(meta, "gate.citation.min", 0)));

        // Rerankers typically want a reasonably-sized candidate pool; use rerankTopK
        // when available.
        int rerankTopK = metaInt(meta, "rerank.topK",
                metaInt(meta, "rerankTopK", metaInt(meta, "rerank_top_k", 0)));
        // Default policy: only skip if we can still return a reasonably sized pool.
        // Start from k (requested docs), then relax to rerankTopK when present (we only
        // need as many as
        // downstream rerank keeps), and finally ensure citations / minimum snippet
        // floor are satisfied.
        int desired = k;
        if (rerankTopK > 0)
            desired = Math.min(desired, rerankTopK);

        desired = Math.max(desired, MIN_SNIPPETS);
        if (minCitations > 0)
            desired = Math.max(desired, minCitations);

        // Never demand more than k for a skip decision.
        desired = Math.min(k, desired);
        return Math.max(1, desired);
    }

    private static int metaInt(java.util.Map<String, Object> meta, String key, int def) {
        if (meta == null)
            return def;
        Object v = meta.get(key);
        if (v instanceof Number n)
            return n.intValue();
        if (v instanceof String s) {
            try {
                return Integer.parseInt(s.trim());
            } catch (Exception ignore) {
            }
        }
        return def;
    }

    private static long metaLong(java.util.Map<String, Object> meta, String key, long def) {
        if (meta == null)
            return def;
        Object v = meta.get(key);
        if (v instanceof Number n)
            return n.longValue();
        if (v instanceof String s) {
            try {
                return Long.parseLong(s.trim());
            } catch (Exception ignore) {
            }
        }
        return def;
    }

    private static double metaDouble(java.util.Map<String, Object> meta, String key, double def) {
        if (meta == null)
            return def;
        Object v = meta.get(key);
        if (v instanceof Number n)
            return n.doubleValue();
        if (v instanceof String s) {
            try {
                return Double.parseDouble(s.trim());
            } catch (Exception ignore) {
            }
        }
        return def;
    }

    private static boolean metaBool(java.util.Map<String, Object> meta, String key, boolean def) {
        if (meta == null)
            return def;
        Object v = meta.get(key);
        if (v instanceof Boolean b)
            return b;
        if (v instanceof Number n)
            return n.intValue() != 0;
        if (v instanceof String s) {
            String t = s.trim().toLowerCase();
            if (t.equals("true") || t.equals("1") || t.equals("yes"))
                return true;
            if (t.equals("false") || t.equals("0") || t.equals("no"))
                return false;
        }
        return def;
    }

    /**
     * TraceStoreì—ì„œ boolean ê°’ì„ ì½ì–´ì˜¤ëŠ” ë„ìš°ë¯¸ ë©”ì„œë“œ.
     * ê¸°ì¡´ metaBool ë¡œì§ì„ ì¬ì‚¬ìš©í•˜ì—¬ íƒ€ì… ë³€í™˜ ë° ê¸°ë³¸ê°’ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
     */
    private static boolean traceBool(String key, boolean def) {
        return metaBool(com.example.lms.search.TraceStore.context(), key, def);
    }

    /**
     * [ECO-FIX v3.0] Aggressive Persistence Loop
     * ë„¤ì´ë²„/ì™¸ë¶€ ê²€ìƒ‰ì´ íƒ€ì„ì•„ì›ƒ(3ì´ˆ) ë˜ëŠ” ì¼ì‹œì  ì¥ì• ë¡œ 0ê±´ì„ ì¤„ ë•Œ,
     * í¬ê¸°í•˜ì§€ ì•Šê³  ìµœëŒ€ 3íšŒê¹Œì§€ ì¬ì‹œë„í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ë³´í•˜ëŠ” ë£¨í”„ì…ë‹ˆë‹¤.
     * ìº¡ì°¨/ë²„ì „/íƒœê·¸ í•„í„°ë¥¼ ë¯¸ë¦¬ ì ìš©í•´ "ì‹¤ì§ˆ ìœ íš¨ ê²°ê³¼" ê¸°ì¤€ìœ¼ë¡œ ì„±ê³µì„ íŒì •í•©ë‹ˆë‹¤.
     */
    private List<String> searchWithAggressiveRetry(String query, int k, java.util.regex.Pattern mustVersion,
            int maxAttempts, long budgetMs) {
        int attempts = Math.max(1, maxAttempts);

        // Backoff policy: short exponential backoff (bounded by overall budget).
        final long baseBackoffMs = 250L;
        final long maxBackoffMs = 1500L;

        final long deadlineMs = (budgetMs > 0 ? System.currentTimeMillis() + budgetMs : Long.MAX_VALUE);

        for (int attempt = 1; attempt <= attempts; attempt++) {
            if (System.currentTimeMillis() > deadlineMs) {
                log.warn("âš ï¸ [WebSearch] budget exhausted ({}ms). Stop retrying. query='{}'", budgetMs, query);
                break;
            }

            long start = System.currentTimeMillis();
            boolean hadRaw = false;

            try {
                List<String> rawResults = webSearchProvider.search(query, k);

                if (rawResults != null && !rawResults.isEmpty()) {
                    hadRaw = true;

                    List<String> valid = rawResults.stream()
                            .filter(s -> !CAPTCHA_HINT.matcher(s).find())
                            .filter(s -> mustVersion == null || mustVersion.matcher(s).find())
                            .filter(s -> {
                                String url = extractUrl(s);
                                if (url == null)
                                    return true;
                                String lower = url.toLowerCase();
                                return !(lower.contains("/tag/") || lower.contains("?tag="));
                            })
                            .toList();

                    if (!valid.isEmpty()) {
                        if (attempt > 1) {
                            log.info("âœ… [WebSearch] Retry success on attempt {}/{} ({}ms). Found {} valid items.",
                                    attempt, attempts, System.currentTimeMillis() - start, valid.size());
                        }
                        return valid;
                    }

                    log.warn(
                            "âš ï¸ [WebSearch] Attempt {}/{} had results but all filtered out (Captcha/Version/Tag).",
                            attempt, attempts);
                } else {
                    log.warn("âš ï¸ [WebSearch] Attempt {}/{} returned 0 results.", attempt, attempts);
                }
            } catch (Exception e) {
                log.warn("ğŸ”¥ [WebSearch] Attempt {}/{} error: {}.", attempt, attempts, e.getMessage());
            }

            // Decide whether a retry is worthwhile.
            if (attempt >= attempts) {
                break;
            }

            // If web is effectively down (both providers down / hybrid down), retries are
            // wasted.
            boolean effectiveDown = traceBool("orch.webRateLimited.effective", false)
                    || traceBool("orch.webRateLimited", false)
                    || traceBool("orch.webRateLimited.allDown", false)
                    || traceBool("orch.webRateLimited.hybridDown", false);

            long skippedCount = 0L;
            try {
                skippedCount = TraceStore.getLong("web.await.skipped.count");
            } catch (Exception ignore) {
            }

            if (effectiveDown || skippedCount >= 2L) {
                log.warn(
                        "â›” [WebSearch] Stop retrying: web effectively down (effectiveDown={}, skippedCount={}). query='{}'",
                        effectiveDown, skippedCount, query);
                break;
            }

            // Retry only when we saw transient signals (timeouts/nonOk) or partial-down.
            long timeoutCount = 0L;
            long timeoutHardCount = 0L;
            long nonOkCount = 0L;
            try {
                timeoutCount = TraceStore.getLong("web.await.events.timeout.count");
                timeoutHardCount = TraceStore.getLong("web.await.events.timeoutHard.count");
                nonOkCount = TraceStore.getLong("web.await.events.nonOk.count");
            } catch (Exception ignore) {
            }

            boolean transientSignal = timeoutCount > 0 || timeoutHardCount > 0 || nonOkCount > 0;
            boolean partialDown = traceBool("orch.webPartialDown", false)
                    || traceBool("orch.webRateLimited.anyDown", false);

            if (!transientSignal && !partialDown && hadRaw) {
                // All filtered out but no transient signals â†’ additional retries rarely help.
                log.warn("â›” [WebSearch] Stop retrying: filtered-out without transient signals. query='{}'", query);
                break;
            }
            if (!transientSignal && !partialDown && !hadRaw) {
                // Pure empty without transient signals â†’ treat as "no results" and stop.
                log.warn("â›” [WebSearch] Stop retrying: empty without transient signals. query='{}'", query);
                break;
            }

            long backoffMs = baseBackoffMs * (1L << Math.min(4, attempt - 1));
            backoffMs = Math.min(maxBackoffMs, backoffMs);

            long remain = deadlineMs - System.currentTimeMillis();
            if (remain <= 0) {
                break;
            }
            backoffMs = Math.min(backoffMs, remain);

            try {
                Thread.sleep(backoffMs);
            } catch (InterruptedException ie) {
                Thread.currentThread().interrupt();
                break;
            }
        }

        log.error("âŒ [WebSearch] All {} attempts failed or produced no valid results for query='{}'. Returning empty.",
                attempts, query);
        return java.util.Collections.emptyList();
    }

    // â”€â”€ URL/source ë©”íƒ€ ë³´ì¡´ì„ ìœ„í•œ URL íŒŒì„œ(Null-safe + ì •ê·œí™”)
    private static String extractUrl(String text) {
        if (text == null)
            return null;
        try {
            int a = text.indexOf("href=\"");
            if (a >= 0) {
                int s = a + 6, e = text.indexOf('"', s);
                if (e > s) {
                    return sanitizeUrl(text.substring(s, e));
                }
            }
        } catch (Exception ignore) {
        }
        try {
            int http = text.indexOf("http");
            if (http >= 0) {
                int sp = text.indexOf(' ', http);
                String raw = sp > http ? text.substring(http, sp) : text.substring(http);
                return sanitizeUrl(raw);
            }
        } catch (Exception ignore) {
        }
        return null;
    }

    /** Trim common trailing punctuation and normalize scheme/quotes. */
    private static String sanitizeUrl(String raw) {
        if (raw == null) {
            return null;
        }
        String u = raw.trim();
        if (u.isEmpty()) {
            return u;
        }
        // Strip common trailing punctuation/brackets that often leak from snippets/log formatting
        while (!u.isEmpty()) {
            char last = u.charAt(u.length() - 1);
            if (last == ')' || last == ']' || last == ',' || last == '.' || last == ';' || last == '"' || last == '\'') {
                u = u.substring(0, u.length() - 1).trim();
                continue;
            }
            break;
        }
        return HtmlTextUtil.normalizeUrl(u);
    }

    /**
     * Build {@link Content} with URL/source metadata preserved.
     *
     * <p>Many downstream components (guard/provenance/TAA) compute evidence diversity using
     * {@code TextSegment.metadata().getString("url"/"source")}. If we only use {@code Content.from(text)},
     * metadata is empty and diversity collapses to 0 even when the snippet contains a URL.
     */
    private Content toWebContent(String text, String url, String providerName) {
        String t = (text == null) ? "" : text;
        String u = sanitizeUrl(url);
        if (u == null || u.isBlank()) {
            // fail-soft: recover from the text itself (e.g., "URL: https://..." or "[ì¶œì²˜] https://...")
            u = extractUrl(t);
        }

        java.util.Map<String, Object> meta = new java.util.LinkedHashMap<>();
        if (u != null && !u.isBlank()) {
            meta.put("url", u);
            // keep legacy key used by parts of the pipeline as "url alternative"
            meta.put("source", u);
        }
        if (providerName != null && !providerName.isBlank()) {
            meta.put("provider", providerName);
        }

        if (meta.isEmpty()) {
            return Content.from(t);
        }
        return Content.from(TextSegment.from(t, Metadata.from(meta)));
    }

    private static java.util.List<String> extractSiteFilters(String query) {
        if (query == null || query.isBlank()) {
            return java.util.Collections.emptyList();
        }
        java.util.regex.Matcher m = SITE_FILTER.matcher(query);
        java.util.LinkedHashSet<String> out = new java.util.LinkedHashSet<>();
        while (m.find()) {
            String raw = m.group(1);
            String norm = normalizeSiteToken(raw);
            if (norm != null && !norm.isBlank()) {
                out.add(norm);
            }
        }
        return new java.util.ArrayList<>(out);
    }

    /**
     * Canonicalizes a query into a "base" form by stripping site: filters and the
     * OR tokens that
     * only exist to join those site filters. This allows us to match:
     *
     * <ul>
     * <li>{@code "foo"}</li>
     * <li>{@code "foo site:example.com"}</li>
     * <li>{@code "foo (site:example.com OR site:example.org)"}</li>
     * </ul>
     */
    private static String canonicalBaseQuery(String query) {
        if (query == null) {
            return "";
        }
        String q = query.trim();
        if (q.isEmpty()) {
            return "";
        }

        // Token-based approach: remove site:* tokens and remove OR tokens only when
        // they are
        // directly adjacent to site tokens (so we don't accidentally drop legitimate
        // query terms).
        String[] parts = q.split("\\s+");
        boolean[] isSite = new boolean[parts.length];
        String[] core = new String[parts.length];

        for (int i = 0; i < parts.length; i++) {
            String t = parts[i];
            if (t == null) {
                core[i] = "";
                isSite[i] = false;
                continue;
            }
            String c = t;
            // strip leading/trailing parentheses to normalize tokens like "(site:..." /
            // "... )"
            while (c.startsWith("("))
                c = c.substring(1);
            while (c.endsWith(")"))
                c = c.substring(0, c.length() - 1);
            core[i] = c;
            isSite[i] = c.toLowerCase(java.util.Locale.ROOT).startsWith("site:");
        }

        java.util.ArrayList<String> keep = new java.util.ArrayList<>();
        for (int i = 0; i < core.length; i++) {
            String tok = core[i];
            if (tok == null || tok.isBlank())
                continue;
            if (isSite[i])
                continue;
            if (tok.equalsIgnoreCase("OR")
                    && ((i > 0 && isSite[i - 1]) || (i + 1 < isSite.length && isSite[i + 1]))) {
                continue;
            }
            keep.add(tok);
        }

        return String.join(" ", keep).trim();
    }

    @SuppressWarnings("unchecked")
    private static java.util.List<String> serpCacheGet(String baseKey) {
        if (baseKey == null || baseKey.isBlank()) {
            return null;
        }
        Object o = com.example.lms.search.TraceStore.get(SERP_CACHE_TRACE_KEY);
        if (o instanceof java.util.Map<?, ?> m) {
            try {
                return ((java.util.Map<String, java.util.List<String>>) m).get(baseKey);
            } catch (ClassCastException ignore) {
                return null;
            }
        }
        return null;
    }

    @SuppressWarnings("unchecked")
    private static void serpCachePut(String baseKey, java.util.List<String> snippets) {
        if (baseKey == null || baseKey.isBlank()) {
            return;
        }
        if (snippets == null || snippets.isEmpty()) {
            return;
        }

        java.util.Map<String, java.util.List<String>> cache;
        Object o = com.example.lms.search.TraceStore.get(SERP_CACHE_TRACE_KEY);
        if (o instanceof java.util.Map<?, ?> m) {
            try {
                cache = (java.util.Map<String, java.util.List<String>>) m;
            } catch (ClassCastException e) {
                cache = new java.util.concurrent.ConcurrentHashMap<>();
                com.example.lms.search.TraceStore.put(SERP_CACHE_TRACE_KEY, cache);
            }
        } else {
            cache = new java.util.concurrent.ConcurrentHashMap<>();
            com.example.lms.search.TraceStore.put(SERP_CACHE_TRACE_KEY, cache);
        }

        java.util.List<String> trimmed = snippets.stream().distinct().limit(SERP_CACHE_MAX).toList();
        cache.put(baseKey, trimmed);
    }

    private static java.util.List<String> filterSnippetsBySites(
            java.util.List<String> snippets,
            java.util.List<String> sites,
            int limit) {
        if (snippets == null || snippets.isEmpty() || sites == null || sites.isEmpty()) {
            return java.util.Collections.emptyList();
        }
        java.util.List<String> normSites = sites.stream()
                .filter(s -> s != null && !s.isBlank())
                .map(String::trim)
                .map(WebSearchRetriever::normalizeSiteToken)
                .filter(s -> s != null && !s.isBlank())
                .distinct()
                .toList();
        if (normSites.isEmpty()) {
            return java.util.Collections.emptyList();
        }

        java.util.LinkedHashSet<String> out = new java.util.LinkedHashSet<>();
        int hardLimit = Math.max(1, limit);

        for (String snip : snippets) {
            if (snip == null || snip.isBlank())
                continue;
            String url = extractUrl(snip);
            if (url == null || url.isBlank())
                continue;
            if (urlMatchesAnySite(url, normSites)) {
                out.add(snip);
                if (out.size() >= hardLimit)
                    break;
            }
        }

        return new java.util.ArrayList<>(out);
    }

    private static boolean urlMatchesAnySite(String url, java.util.List<String> sites) {
        String host = null;
        try {
            host = java.net.URI.create(url).getHost();
        } catch (Exception ignore) {
            // fall through
        }
        if (host == null || host.isBlank()) {
            try {
                String s = url;
                s = s.replaceFirst("^https?://", "");
                int cut = s.indexOf('/');
                if (cut >= 0)
                    s = s.substring(0, cut);
                cut = s.indexOf('?');
                if (cut >= 0)
                    s = s.substring(0, cut);
                cut = s.indexOf('#');
                if (cut >= 0)
                    s = s.substring(0, cut);
                host = s;
            } catch (Exception ignore2) {
                host = null;
            }
        }
        if (host == null || host.isBlank()) {
            return false;
        }
        String h = host.toLowerCase(java.util.Locale.ROOT);
        for (String site : sites) {
            if (site == null || site.isBlank())
                continue;
            String s = site.toLowerCase(java.util.Locale.ROOT);
            if (h.equals(s) || h.endsWith("." + s) || h.endsWith(s)) {
                return true;
            }
        }
        return false;
    }

    private static String normalizeSiteToken(String raw) {
        if (raw == null) {
            return null;
        }
        String s = raw.trim();
        if (s.isEmpty()) {
            return null;
        }
        s = s.replaceFirst("^https?://", "");
        // trim path
        int slash = s.indexOf('/');
        if (slash >= 0) {
            s = s.substring(0, slash);
        }
        // strip leading www.
        s = s.replaceFirst("^www\\.", "");
        return s.trim();
    }

    // â”€â”€ NEW: SnippetPruner ì—†ì´ë„ ë™ì‘í•˜ëŠ” ê²½ëŸ‰ ë”¥ ìŠ¤ë‹ˆí« ì¶”ì¶œê¸°
    private static String pickByHeuristic(String q, String body, int maxLen) {
        if (body == null || body.isBlank())
            return "";
        if (q == null)
            q = "";
        String[] toks = q.toLowerCase().split("\\s+");
        String[] sents = body.split("(?<=[\\.\\?\\!ã€‚ï¼ï¼Ÿ])\\s+");
        String best = "";
        int bestScore = -1;
        for (String s : sents) {
            if (s == null || s.isBlank())
                continue;
            String ls = s.toLowerCase();
            int score = 0;
            for (String t : toks) {
                if (t.isBlank())
                    continue;
                if (ls.contains(t))
                    score += 2; // ì§ˆì˜ í† í° í¬í•¨ ê°€ì¤‘
            }
            score += Math.min(s.length(), 300) / 60; // ë¬¸ì¥ ê¸¸ì´ ê°€ì¤‘(ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ íŒ¨ë„í‹°)
            if (score > bestScore) {
                bestScore = score;
                best = s.trim();
            }
        }
        if (best.isEmpty()) {
            best = body.length() > maxLen ? body.substring(0, maxLen) : body;
        } else if (best.length() > maxLen) {
            best = best.substring(0, maxLen) + "/* ... *&#47;";
        }
        return best;
    }
}

// PATCH_MARKER: WebSearchRetriever updated per latest spec.
