package com.example.lms.service.rag;
import java.util.regex.Pattern;              /* ğŸ”´ NEW */
import com.example.lms.service.NaverSearchService;
import dev.langchain4j.rag.content.Content;
import dev.langchain4j.rag.content.retriever.ContentRetriever;
import dev.langchain4j.rag.query.Query;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import java.io.IOException;
import java.util.*;
import java.util.regex.Pattern;              /* ğŸ”´ NEW */
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * í˜•íƒœì†Œ ë¶„ì„ â†’ í‚¤ì›Œë“œ ê¸°ë°˜ ë„¤ì´ë²„ ê²€ìƒ‰ Retriever.
 */
@Slf4j
@RequiredArgsConstructor
public class AnalyzeWebSearchRetriever implements ContentRetriever {

    private final Analyzer           analyzer;
    private final NaverSearchService searchSvc;
    private final int                topK;

    /** Executor for parallel token searches */
    private final ExecutorService searchExecutor = Executors.newFixedThreadPool(
            Math.max(2, Runtime.getRuntime().availableProcessors())
    );
    /** per-token ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ(ms) */
    private static final long PER_TOKEN_TIMEOUT_MS = 5000L;

    /* ğŸ”´ ë©”íƒ€ íƒœê·¸â€§ì‹œê°„ íƒœê·¸ í•„í„°ìš© íŒ¨í„´ */
    private static final Pattern META_TAG = Pattern.compile("\\[[^\\]]+\\]");
    private static final Pattern TIME_TAG = Pattern.compile("\\b\\d{1,2}:\\d{2}\\b");

    /* ğŸ”´ ë©”íƒ€Â·ê°œí–‰ ì œê±° ìœ í‹¸ */
    private static String normalize(String raw) {
        if (raw == null) return "";
        String s = META_TAG.matcher(raw).replaceAll("");
        s = TIME_TAG.matcher(s).replaceAll("");
        return s.replace("\n", " ").trim();
    }

    @Override
    public List<Content> retrieve(Query query) {

        /* ğŸ”´ 1) ì›ë¬¸ ì •ê·œí™”(ë…¸ì´ì¦ˆ ì œê±°) */
        String normalized = normalize(query.text());

        /* 2) í˜•íƒœì†Œ í† í°í™” */
        Set<String> tokens = analyze(normalized);
        if (tokens.isEmpty()) return List.of();

        /* 3) í† í°ë³„ ê²€ìƒ‰ í›„ í•©ì¹˜ê¸° */
        int eachTopK = Math.max(1, topK / tokens.size());
        List<Content> merged = new ArrayList<>();

        // perform searches for each token in parallel
        List<CompletableFuture<List<Content>>> futures = new ArrayList<>();
        for (String t : tokens) {
            futures.add(
                    CompletableFuture.supplyAsync(() -> {
                        List<String> lines = searchSvc.searchSnippets(t, eachTopK);
                        return lines.stream().map(Content::from).toList();
                    }, searchExecutor)
            );
        }
        for (CompletableFuture<List<Content>> future : futures) {
            try {
                merged.addAll(future.get(PER_TOKEN_TIMEOUT_MS, TimeUnit.MILLISECONDS));
            } catch (TimeoutException te) {
                future.cancel(true);
                log.warn("[Analyze] token search timed out ({} ms)", PER_TOKEN_TIMEOUT_MS);
            } catch (Exception e) {
                log.warn("[Analyze] async search failed", e);
            }
            if (merged.size() >= topK) {
                break;
            }
        }
        return merged.size() > topK ? merged.subList(0, topK) : merged;
    }

    /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    private Set<String> analyze(String text) {
        Set<String> terms = new LinkedHashSet<>();
        try (TokenStream ts = analyzer.tokenStream("f", text)) {
            CharTermAttribute attr = ts.addAttribute(CharTermAttribute.class);
            ts.reset();
            while (ts.incrementToken()) {
                String term = attr.toString().trim();
                if (term.length() > 1) {
                    terms.add(term);   // í•œ ê¸€ì í† í°ë§Œ ì œì™¸, ë‘ ê¸€ì ì´ìƒì€ í¬í•¨
                }
            }
            ts.end();
        } catch (IOException e) {
            log.warn("[Analyze] Tokenizing failed: {}", e.getMessage(), e);
        } catch (Exception e) {
            log.error("[Analyze] Unexpected error", e);
        }
        return terms;
    }
}