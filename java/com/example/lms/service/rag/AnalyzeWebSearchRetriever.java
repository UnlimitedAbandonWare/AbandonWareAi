package com.example.lms.service.rag;
import java.util.regex.Pattern;              /* ğŸ”´ NEW */
import com.example.lms.service.NaverSearchService;
import dev.langchain4j.rag.content.Content;
import dev.langchain4j.rag.content.retriever.ContentRetriever;
import dev.langchain4j.rag.query.Query;
// lombok.RequiredArgsConstructor;      // â† import ì œê±°
import lombok.extern.slf4j.Slf4j;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import java.util.concurrent.ExecutorService;   // âœ… ì¶”ê°€
import java.util.concurrent.Executors;        // âœ… ì¶”ê°€

import java.io.IOException;
import java.util.*;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * í˜•íƒœì†Œ ë¶„ì„ â†’ í‚¤ì›Œë“œ ê¸°ë°˜ ë„¤ì´ë²„ ê²€ìƒ‰ Retriever.
 */
@Slf4j
public class AnalyzeWebSearchRetriever implements ContentRetriever {

    private final Analyzer           analyzer;
    private final NaverSearchService searchSvc;
    private final int                maxTokens;  // í† í°í™” ê¸¸ì´ ì œí•œ
    private final int                topK;       // ê²€ìƒ‰ ê²°ê³¼ ìƒí•œ (maxTokensì™€ ë™ì¼ ê°’ ì‚¬ìš©)
    /** Executor for parallel token searches */
    private final ExecutorService searchExecutor =
            Executors.newFixedThreadPool(
                    Math.max(2, Runtime.getRuntime().availableProcessors()));


    public AnalyzeWebSearchRetriever(
            Analyzer analyzer,
            NaverSearchService svc,
            int maxTokens) {
        this.analyzer = analyzer;
        this.searchSvc = svc;
        this.maxTokens = maxTokens;
        this.topK = maxTokens;   // ë™ì¼ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
    }

    /* ğŸ”´ ë©”íƒ€ íƒœê·¸â€§ì‹œê°„ íƒœê·¸ í•„í„°ìš© íŒ¨í„´ */
    private static final Pattern META_TAG = Pattern.compile("\\[[^\\]]+\\]");
    private static final Pattern TIME_TAG = Pattern.compile("\\b\\d{1,2}:\\d{2}\\b");

    /* ğŸ”´ ì˜ë¬¸í•œê¸€/ê³ ìœ ëª…ì‚¬ ê²°í•© ë³´ì¡´ ê·œì¹™ */
    private static final Pattern PROPER_COMPOUND = Pattern.compile(
            "(?i)^[A-Z]{1,6}[\\s-]?[ê°€-í£].*|[A-Za-z]ì•„ì¹´ë°ë¯¸$|[ê°€-í£]ì•„ì¹´ë°ë¯¸$");

    /* ğŸ”´ ë©”íƒ€Â·ê°œí–‰ ì œê±° ìœ í‹¸ */
    private static String normalize(String raw) {
        if (raw == null) return "";
        String s = META_TAG.matcher(raw).replaceAll("");
        s = TIME_TAG.matcher(s).replaceAll("");
        return s.replace("\n", " ").trim();
    }

    @Override
    public List<Content> retrieve(Query query) {

        /* ğŸ”´ 1) ì›ë¬¸ ì •ê·œí™”(ë…¸ì´ì¦ˆ ì œê±°) */
        String normalized = normalize(query.text());
        /* 2) ê³ ìœ ëª…ì‚¬ íŒ¨í„´ì´ë©´ ë¶„í•´ ìš°íšŒ(ì›ë¬¸ ê·¸ëŒ€ë¡œ ê²€ìƒ‰) */
        if (PROPER_COMPOUND.matcher(normalized).find()) {
            List<String> lines = searchSvc.searchSnippets(normalized, topK);
            return lines.stream().distinct().limit(topK).map(Content::from).toList();
        }


        /* 2) í˜•íƒœì†Œ í† í°í™” */
        Set<String> tokens = analyze(normalized);
        if (tokens.isEmpty()) return List.of();

        /* 3) í† í°ë³„ ê²€ìƒ‰ í›„ í•©ì¹˜ê¸° */
        int eachTopK = Math.max(1, topK / tokens.size());
        List<Content> merged = new ArrayList<>();

        // perform searches for each token in parallel
        List<CompletableFuture<List<Content>>> futures = new ArrayList<>();
        for (String t : tokens) {
            futures.add(
                    CompletableFuture.supplyAsync(() -> {
                        List<String> lines = searchSvc.searchSnippets(t, eachTopK);
                        return lines.stream().map(Content::from).toList();
                    }, searchExecutor)
            );
        }
        for (CompletableFuture<List<Content>> future : futures) {
            try {
                merged.addAll(future.join());
            } catch (Exception e) {
                log.warn("[Analyze] async search failed", e);
            }
            if (merged.size() >= topK) {
                break;
            }
        }
        return merged.size() > topK ? merged.subList(0, topK) : merged;
    }

    /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    private Set<String> analyze(String text) {
        Set<String> terms = new LinkedHashSet<>();
        try (TokenStream ts = analyzer.tokenStream("f", text)) {
            CharTermAttribute attr = ts.addAttribute(CharTermAttribute.class);
            ts.reset();
            while (ts.incrementToken()) {
                String term = attr.toString().trim();
                if (term.length() > 1) {
                    terms.add(term);   // í•œ ê¸€ì í† í°ë§Œ ì œì™¸, ë‘ ê¸€ì ì´ìƒì€ í¬í•¨
                }
            }
            ts.end();
        } catch (IOException e) {
            log.warn("[Analyze] Tokenizing failed: {}", e.getMessage(), e);
        } catch (Exception e) {
            log.error("[Analyze] Unexpected error", e);
        }
        return terms;
    }
}